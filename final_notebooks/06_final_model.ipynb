{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## FINAL MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Huawei\n",
      "[nltk_data]     D15\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 1;\n                var nbb_unformatted_code = \"# importing dependencies here\\nimport numpy as np\\nimport pandas as pd\\nimport os\\n\\n# visualizations\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# feature engineering\\nimport re\\nimport nltk\\nfrom nltk.stem import WordNetLemmatizer\\nfrom nltk.corpus import stopwords\\n\\nnltk.download(\\\"stopwords\\\")\\n\\n# sentiment scoring\\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\\n# pos tagging\\nimport nltk\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\n\\n# accuracy scores\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n)\\n\\n# performance check\\nimport time\\n\\n# sparse to dense\\nfrom sklearn.base import TransformerMixin\\n\\n\\nclass DenseTransformer(TransformerMixin):\\n    def fit(self, X, y=None, **fit_params):\\n        return self\\n\\n    def transform(self, X, y=None, **fit_params):\\n        return X.todense()\\n\\n\\n# importing model\\nfrom joblib import load\\n\\n# code formatter\\n%load_ext nb_black\";\n                var nbb_formatted_code = \"# importing dependencies here\\nimport numpy as np\\nimport pandas as pd\\nimport os\\n\\n# visualizations\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# feature engineering\\nimport re\\nimport nltk\\nfrom nltk.stem import WordNetLemmatizer\\nfrom nltk.corpus import stopwords\\n\\nnltk.download(\\\"stopwords\\\")\\n\\n# sentiment scoring\\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\\n# pos tagging\\nimport nltk\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\n\\n# accuracy scores\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n)\\n\\n# performance check\\nimport time\\n\\n# sparse to dense\\nfrom sklearn.base import TransformerMixin\\n\\n\\nclass DenseTransformer(TransformerMixin):\\n    def fit(self, X, y=None, **fit_params):\\n        return self\\n\\n    def transform(self, X, y=None, **fit_params):\\n        return X.todense()\\n\\n\\n# importing model\\nfrom joblib import load\\n\\n# code formatter\\n%load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing dependencies here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# feature engineering\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# sentiment scoring\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# pos tagging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# accuracy scores\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "\n",
    "# performance check\n",
    "import time\n",
    "\n",
    "# sparse to dense\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "\n",
    "# importing model\n",
    "from joblib import load\n",
    "\n",
    "# code formatter\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting for the Holdout Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"# reading the test dataset\\npath_to_csv = os.path.join(\\\"..\\\", \\\"data\\\", \\\"df_holdout.csv\\\")\\ndf = pd.read_csv(path_to_csv)\";\n                var nbb_formatted_code = \"# reading the test dataset\\npath_to_csv = os.path.join(\\\"..\\\", \\\"data\\\", \\\"df_holdout.csv\\\")\\ndf = pd.read_csv(path_to_csv)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reading the test dataset\n",
    "path_to_csv = os.path.join(\"..\", \"data\", \"df_holdout.csv\")\n",
    "df = pd.read_csv(path_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'I have never seen so many poorly used memes.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'Wow! You are obviously her muse... Be flatter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'I have never seen so many poorly used memes.....\n",
       "1  INFJ  'Wow! You are obviously her muse... Be flatter..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 3;\n                var nbb_unformatted_code = \"# checking top records\\ndf.head(2)\";\n                var nbb_formatted_code = \"# checking top records\\ndf.head(2)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking top records\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"def categorize_types(personality_data):\\n\\n    personality_data[\\\"is_Extrovert\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[0] == \\\"E\\\" else 0\\n    )\\n    personality_data[\\\"is_Sensing\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[1] == \\\"S\\\" else 0\\n    )\\n    personality_data[\\\"is_Thinking\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[2] == \\\"T\\\" else 0\\n    )\\n    personality_data[\\\"is_Judging\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[3] == \\\"J\\\" else 0\\n    )\\n\\n    # rearranging the dataframe columns\\n    personality_data = personality_data[\\n        [\\\"type\\\", \\\"is_Extrovert\\\", \\\"is_Sensing\\\", \\\"is_Thinking\\\", \\\"is_Judging\\\", \\\"posts\\\"]\\n    ]\\n\\n\\n#######################################################################################################3\\n\\n\\ndef clean_posts(personality_data):\\n\\n    # converting posts into lower case\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"posts\\\"].str.lower()\\n\\n    # replacing ||| with space\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\|\\\\|\\\\|\\\"), \\\" \\\"\\n    )\\n\\n    # replacing urls with domain name\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        \\\"\\\"\\n    )\\n\\n    # dropping emails\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\S+@\\\\S+\\\"), \\\"\\\"\\n    )\\n\\n    # dropping punctuations\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"[^a-z\\\\s]\\\"), \\\" \\\"\\n    )\\n\\n    # dropping MBTIs mentioned in the posts. There are quite a few mention of these types in these posts.\\n    mbti = personality_data[\\\"type\\\"].unique()\\n    for type_word in mbti:\\n        personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n            type_word.lower(), \\\"\\\"\\n        )\\n        \\n    # removing words that are 1 to 2 characters long    \\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n    re.compile(r\\\"\\\\b\\\\w{1,2}\\\\b\\\"), \\\"\\\"\\n    )\\n\\n    # lemmitizing\\n    lemmatizer = WordNetLemmatizer()\\n\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].apply(\\n        lambda x: \\\" \\\".join(\\n            [\\n                lemmatizer.lemmatize(word)\\n                for word in x.split(\\\" \\\")\\n                if word not in stopwords.words(\\\"english\\\")\\n            ]\\n        )\\n    )\\n\\n    # tag_posts will be a list of 50 lists. need it for word stats (per post for each user)\\n    # replacing urls with domain name\\n    personality_data[\\\"tag_posts\\\"] = personality_data[\\\"posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        lambda match: match.group(2),\\n    )\\n    # replacing ||| with space\\n    personality_data[\\\"tag_posts\\\"] = [\\n        post for post in personality_data[\\\"tag_posts\\\"].str.split(\\\"\\\\|\\\\|\\\\|\\\")\\n    ]\\n\\n\\n#################################################################################################################\\n\\n\\ndef sentiment_score(personality_data):\\n\\n    analyzer = SentimentIntensityAnalyzer()\\n\\n    nlp_sentiment_score = []\\n\\n    for post in personality_data[\\\"clean_posts\\\"]:\\n        score = analyzer.polarity_scores(post)[\\\"compound\\\"]\\n        nlp_sentiment_score.append(score)\\n\\n    personality_data[\\\"compound_sentiment\\\"] = nlp_sentiment_score\\n\\n\\n###############################################################################################################\\n\\n\\ndef pos_tagging(personality_data):\\n\\n    personality_data[\\\"tagged_words\\\"] = personality_data[\\\"tag_posts\\\"].apply(\\n        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\\n    )\\n\\n    # grouping pos tags based on stanford list\\n    tags_dict = {\\n        \\\"ADJ\\\": [\\\"JJ\\\", \\\"JJR\\\", \\\"JJS\\\"],\\n        \\\"ADP\\\": [\\\"EX\\\", \\\"TO\\\"],\\n        \\\"ADV\\\": [\\\"RB\\\", \\\"RBR\\\", \\\"RBS\\\", \\\"WRB\\\"],\\n        \\\"CONJ\\\": [\\\"CC\\\", \\\"IN\\\"],\\n        \\\"DET\\\": [\\\"DT\\\", \\\"PDT\\\", \\\"WDT\\\"],\\n        \\\"NOUN\\\": [\\\"NN\\\", \\\"NNS\\\", \\\"NNP\\\", \\\"NNPS\\\"],\\n        \\\"NUM\\\": [\\\"CD\\\"],\\n        \\\"PRT\\\": [\\\"RP\\\"],\\n        \\\"PRON\\\": [\\\"PRP\\\", \\\"PRP$\\\", \\\"WP\\\", \\\"WP$\\\"],\\n        \\\"VERB\\\": [\\\"MD\\\", \\\"VB\\\", \\\"VBD\\\", \\\"VBG\\\", \\\"VBN\\\", \\\"VBP\\\", \\\"VBZ\\\"],\\n        \\\".\\\": [\\\"#\\\", \\\"$\\\", \\\"''\\\", \\\"(\\\", \\\")\\\", \\\",\\\", \\\".\\\", \\\":\\\"],\\n        \\\"X\\\": [\\\"FW\\\", \\\"LS\\\", \\\"UH\\\"],\\n    }\\n\\n    def stanford_tag(x, tag):\\n        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\\n        return tags_list\\n\\n    for col in tags_dict.keys():\\n        personality_data[col + \\\"_avg\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.mean(stanford_tag(x, col))\\n        )\\n\\n\\n###############################################################################################################\\n\\n\\ndef get_counts(personality_data):\\n    def unique_words(s):\\n        unique = set(s.split(\\\" \\\"))\\n        return len(unique)/50\\n\\n    def emojis(post):\\n        # does not include emojis made purely from symbols, only :word:\\n        emoji_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                if e.count(\\\":\\\") == 2:\\n                    emoji_count += 1\\n        return emoji_count/50\\n\\n    def colons(post):\\n        # Includes colons used in emojis\\n        colon_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                colon_count += e.count(\\\":\\\")\\n        return colon_count/50\\n\\n    personality_data[\\\"qm\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"?\\\")/50)\\n    personality_data[\\\"em\\\"] = personality_data[\\\"posts\\\"].apply(lambda s: s.count(\\\"!\\\")/50)\\n    personality_data[\\\"colons\\\"] = personality_data[\\\"posts\\\"].apply(colons)\\n    personality_data[\\\"emojis\\\"] = personality_data[\\\"posts\\\"].apply(emojis)\\n\\n    personality_data[\\\"word_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: (s.count(\\\" \\\") + 1)/50\\n    )\\n    personality_data[\\\"unique_words\\\"] = personality_data[\\\"posts\\\"].apply(unique_words)\\n\\n    personality_data[\\\"upper\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda x: len([x for x in x.split() if x.isupper()])/50\\n    )\\n    personality_data[\\\"link_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\"http\\\")/50\\n    )\\n    ellipses_count = [\\n        len(re.findall(r\\\"\\\\.\\\\.\\\\.\\\\ \\\", posts))/50 for posts in personality_data[\\\"posts\\\"]\\n    ]\\n    personality_data[\\\"ellipses\\\"] = ellipses_count\\n    personality_data[\\\"img_count\\\"] = [\\n        len(re.findall(r\\\"(\\\\.jpg)|(\\\\.jpeg)|(\\\\.gif)|(\\\\.png)\\\", post))/50\\n        for post in personality_data[\\\"posts\\\"]\\n    ]\";\n                var nbb_formatted_code = \"def categorize_types(personality_data):\\n\\n    personality_data[\\\"is_Extrovert\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[0] == \\\"E\\\" else 0\\n    )\\n    personality_data[\\\"is_Sensing\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[1] == \\\"S\\\" else 0\\n    )\\n    personality_data[\\\"is_Thinking\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[2] == \\\"T\\\" else 0\\n    )\\n    personality_data[\\\"is_Judging\\\"] = personality_data[\\\"type\\\"].apply(\\n        lambda x: 1 if x[3] == \\\"J\\\" else 0\\n    )\\n\\n    # rearranging the dataframe columns\\n    personality_data = personality_data[\\n        [\\\"type\\\", \\\"is_Extrovert\\\", \\\"is_Sensing\\\", \\\"is_Thinking\\\", \\\"is_Judging\\\", \\\"posts\\\"]\\n    ]\\n\\n\\n#######################################################################################################3\\n\\n\\ndef clean_posts(personality_data):\\n\\n    # converting posts into lower case\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"posts\\\"].str.lower()\\n\\n    # replacing ||| with space\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\|\\\\|\\\\|\\\"), \\\" \\\"\\n    )\\n\\n    # replacing urls with domain name\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"), \\\"\\\"\\n    )\\n\\n    # dropping emails\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\S+@\\\\S+\\\"), \\\"\\\"\\n    )\\n\\n    # dropping punctuations\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"[^a-z\\\\s]\\\"), \\\" \\\"\\n    )\\n\\n    # dropping MBTIs mentioned in the posts. There are quite a few mention of these types in these posts.\\n    mbti = personality_data[\\\"type\\\"].unique()\\n    for type_word in mbti:\\n        personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n            type_word.lower(), \\\"\\\"\\n        )\\n\\n    # removing words that are 1 to 2 characters long\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].str.replace(\\n        re.compile(r\\\"\\\\b\\\\w{1,2}\\\\b\\\"), \\\"\\\"\\n    )\\n\\n    # lemmitizing\\n    lemmatizer = WordNetLemmatizer()\\n\\n    personality_data[\\\"clean_posts\\\"] = personality_data[\\\"clean_posts\\\"].apply(\\n        lambda x: \\\" \\\".join(\\n            [\\n                lemmatizer.lemmatize(word)\\n                for word in x.split(\\\" \\\")\\n                if word not in stopwords.words(\\\"english\\\")\\n            ]\\n        )\\n    )\\n\\n    # tag_posts will be a list of 50 lists. need it for word stats (per post for each user)\\n    # replacing urls with domain name\\n    personality_data[\\\"tag_posts\\\"] = personality_data[\\\"posts\\\"].str.replace(\\n        re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+)([\\\\S])*\\\"),\\n        lambda match: match.group(2),\\n    )\\n    # replacing ||| with space\\n    personality_data[\\\"tag_posts\\\"] = [\\n        post for post in personality_data[\\\"tag_posts\\\"].str.split(\\\"\\\\|\\\\|\\\\|\\\")\\n    ]\\n\\n\\n#################################################################################################################\\n\\n\\ndef sentiment_score(personality_data):\\n\\n    analyzer = SentimentIntensityAnalyzer()\\n\\n    nlp_sentiment_score = []\\n\\n    for post in personality_data[\\\"clean_posts\\\"]:\\n        score = analyzer.polarity_scores(post)[\\\"compound\\\"]\\n        nlp_sentiment_score.append(score)\\n\\n    personality_data[\\\"compound_sentiment\\\"] = nlp_sentiment_score\\n\\n\\n###############################################################################################################\\n\\n\\ndef pos_tagging(personality_data):\\n\\n    personality_data[\\\"tagged_words\\\"] = personality_data[\\\"tag_posts\\\"].apply(\\n        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\\n    )\\n\\n    # grouping pos tags based on stanford list\\n    tags_dict = {\\n        \\\"ADJ\\\": [\\\"JJ\\\", \\\"JJR\\\", \\\"JJS\\\"],\\n        \\\"ADP\\\": [\\\"EX\\\", \\\"TO\\\"],\\n        \\\"ADV\\\": [\\\"RB\\\", \\\"RBR\\\", \\\"RBS\\\", \\\"WRB\\\"],\\n        \\\"CONJ\\\": [\\\"CC\\\", \\\"IN\\\"],\\n        \\\"DET\\\": [\\\"DT\\\", \\\"PDT\\\", \\\"WDT\\\"],\\n        \\\"NOUN\\\": [\\\"NN\\\", \\\"NNS\\\", \\\"NNP\\\", \\\"NNPS\\\"],\\n        \\\"NUM\\\": [\\\"CD\\\"],\\n        \\\"PRT\\\": [\\\"RP\\\"],\\n        \\\"PRON\\\": [\\\"PRP\\\", \\\"PRP$\\\", \\\"WP\\\", \\\"WP$\\\"],\\n        \\\"VERB\\\": [\\\"MD\\\", \\\"VB\\\", \\\"VBD\\\", \\\"VBG\\\", \\\"VBN\\\", \\\"VBP\\\", \\\"VBZ\\\"],\\n        \\\".\\\": [\\\"#\\\", \\\"$\\\", \\\"''\\\", \\\"(\\\", \\\")\\\", \\\",\\\", \\\".\\\", \\\":\\\"],\\n        \\\"X\\\": [\\\"FW\\\", \\\"LS\\\", \\\"UH\\\"],\\n    }\\n\\n    def stanford_tag(x, tag):\\n        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\\n        return tags_list\\n\\n    for col in tags_dict.keys():\\n        personality_data[col + \\\"_avg\\\"] = personality_data[\\\"tagged_words\\\"].apply(\\n            lambda x: np.mean(stanford_tag(x, col))\\n        )\\n\\n\\n###############################################################################################################\\n\\n\\ndef get_counts(personality_data):\\n    def unique_words(s):\\n        unique = set(s.split(\\\" \\\"))\\n        return len(unique) / 50\\n\\n    def emojis(post):\\n        # does not include emojis made purely from symbols, only :word:\\n        emoji_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                if e.count(\\\":\\\") == 2:\\n                    emoji_count += 1\\n        return emoji_count / 50\\n\\n    def colons(post):\\n        # Includes colons used in emojis\\n        colon_count = 0\\n        words = post.split()\\n        for e in words:\\n            if \\\"http\\\" not in e:\\n                colon_count += e.count(\\\":\\\")\\n        return colon_count / 50\\n\\n    personality_data[\\\"qm\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\"?\\\") / 50\\n    )\\n    personality_data[\\\"em\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\"!\\\") / 50\\n    )\\n    personality_data[\\\"colons\\\"] = personality_data[\\\"posts\\\"].apply(colons)\\n    personality_data[\\\"emojis\\\"] = personality_data[\\\"posts\\\"].apply(emojis)\\n\\n    personality_data[\\\"word_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: (s.count(\\\" \\\") + 1) / 50\\n    )\\n    personality_data[\\\"unique_words\\\"] = personality_data[\\\"posts\\\"].apply(unique_words)\\n\\n    personality_data[\\\"upper\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda x: len([x for x in x.split() if x.isupper()]) / 50\\n    )\\n    personality_data[\\\"link_count\\\"] = personality_data[\\\"posts\\\"].apply(\\n        lambda s: s.count(\\\"http\\\") / 50\\n    )\\n    ellipses_count = [\\n        len(re.findall(r\\\"\\\\.\\\\.\\\\.\\\\ \\\", posts)) / 50 for posts in personality_data[\\\"posts\\\"]\\n    ]\\n    personality_data[\\\"ellipses\\\"] = ellipses_count\\n    personality_data[\\\"img_count\\\"] = [\\n        len(re.findall(r\\\"(\\\\.jpg)|(\\\\.jpeg)|(\\\\.gif)|(\\\\.png)\\\", post)) / 50\\n        for post in personality_data[\\\"posts\\\"]\\n    ]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def categorize_types(personality_data):\n",
    "\n",
    "    personality_data[\"is_Extrovert\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[0] == \"E\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Sensing\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[1] == \"S\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Thinking\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[2] == \"T\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Judging\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[3] == \"J\" else 0\n",
    "    )\n",
    "\n",
    "    # rearranging the dataframe columns\n",
    "    personality_data = personality_data[\n",
    "        [\"type\", \"is_Extrovert\", \"is_Sensing\", \"is_Thinking\", \"is_Judging\", \"posts\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "#######################################################################################################3\n",
    "\n",
    "\n",
    "def clean_posts(personality_data):\n",
    "\n",
    "    # converting posts into lower case\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"posts\"].str.lower()\n",
    "\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"\\|\\|\\|\"), \" \"\n",
    "    )\n",
    "\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "        \"\"\n",
    "    )\n",
    "\n",
    "    # dropping emails\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"\\S+@\\S+\"), \"\"\n",
    "    )\n",
    "\n",
    "    # dropping punctuations\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        re.compile(r\"[^a-z\\s]\"), \" \"\n",
    "    )\n",
    "\n",
    "    # dropping MBTIs mentioned in the posts. There are quite a few mention of these types in these posts.\n",
    "    mbti = personality_data[\"type\"].unique()\n",
    "    for type_word in mbti:\n",
    "        personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "            type_word.lower(), \"\"\n",
    "        )\n",
    "        \n",
    "    # removing words that are 1 to 2 characters long    \n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "    re.compile(r\"\\b\\w{1,2}\\b\"), \"\"\n",
    "    )\n",
    "\n",
    "    # lemmitizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].apply(\n",
    "        lambda x: \" \".join(\n",
    "            [\n",
    "                lemmatizer.lemmatize(word)\n",
    "                for word in x.split(\" \")\n",
    "                if word not in stopwords.words(\"english\")\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # tag_posts will be a list of 50 lists. need it for word stats (per post for each user)\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"tag_posts\"] = personality_data[\"posts\"].str.replace(\n",
    "        re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "        lambda match: match.group(2),\n",
    "    )\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"tag_posts\"] = [\n",
    "        post for post in personality_data[\"tag_posts\"].str.split(\"\\|\\|\\|\")\n",
    "    ]\n",
    "\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "\n",
    "def sentiment_score(personality_data):\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    nlp_sentiment_score = []\n",
    "\n",
    "    for post in personality_data[\"clean_posts\"]:\n",
    "        score = analyzer.polarity_scores(post)[\"compound\"]\n",
    "        nlp_sentiment_score.append(score)\n",
    "\n",
    "    personality_data[\"compound_sentiment\"] = nlp_sentiment_score\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "\n",
    "def pos_tagging(personality_data):\n",
    "\n",
    "    personality_data[\"tagged_words\"] = personality_data[\"tag_posts\"].apply(\n",
    "        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\n",
    "    )\n",
    "\n",
    "    # grouping pos tags based on stanford list\n",
    "    tags_dict = {\n",
    "        \"ADJ\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "        \"ADP\": [\"EX\", \"TO\"],\n",
    "        \"ADV\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "        \"CONJ\": [\"CC\", \"IN\"],\n",
    "        \"DET\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "        \"NOUN\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "        \"NUM\": [\"CD\"],\n",
    "        \"PRT\": [\"RP\"],\n",
    "        \"PRON\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "        \"VERB\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "        \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "        \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "    }\n",
    "\n",
    "    def stanford_tag(x, tag):\n",
    "        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\n",
    "        return tags_list\n",
    "\n",
    "    for col in tags_dict.keys():\n",
    "        personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
    "            lambda x: np.mean(stanford_tag(x, col))\n",
    "        )\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "\n",
    "def get_counts(personality_data):\n",
    "    def unique_words(s):\n",
    "        unique = set(s.split(\" \"))\n",
    "        return len(unique)/50\n",
    "\n",
    "    def emojis(post):\n",
    "        # does not include emojis made purely from symbols, only :word:\n",
    "        emoji_count = 0\n",
    "        words = post.split()\n",
    "        for e in words:\n",
    "            if \"http\" not in e:\n",
    "                if e.count(\":\") == 2:\n",
    "                    emoji_count += 1\n",
    "        return emoji_count/50\n",
    "\n",
    "    def colons(post):\n",
    "        # Includes colons used in emojis\n",
    "        colon_count = 0\n",
    "        words = post.split()\n",
    "        for e in words:\n",
    "            if \"http\" not in e:\n",
    "                colon_count += e.count(\":\")\n",
    "        return colon_count/50\n",
    "\n",
    "    personality_data[\"qm\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"?\")/50)\n",
    "    personality_data[\"em\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"!\")/50)\n",
    "    personality_data[\"colons\"] = personality_data[\"posts\"].apply(colons)\n",
    "    personality_data[\"emojis\"] = personality_data[\"posts\"].apply(emojis)\n",
    "\n",
    "    personality_data[\"word_count\"] = personality_data[\"posts\"].apply(\n",
    "        lambda s: (s.count(\" \") + 1)/50\n",
    "    )\n",
    "    personality_data[\"unique_words\"] = personality_data[\"posts\"].apply(unique_words)\n",
    "\n",
    "    personality_data[\"upper\"] = personality_data[\"posts\"].apply(\n",
    "        lambda x: len([x for x in x.split() if x.isupper()])/50\n",
    "    )\n",
    "    personality_data[\"link_count\"] = personality_data[\"posts\"].apply(\n",
    "        lambda s: s.count(\"http\")/50\n",
    "    )\n",
    "    ellipses_count = [\n",
    "        len(re.findall(r\"\\.\\.\\.\\ \", posts))/50 for posts in personality_data[\"posts\"]\n",
    "    ]\n",
    "    personality_data[\"ellipses\"] = ellipses_count\n",
    "    personality_data[\"img_count\"] = [\n",
    "        len(re.findall(r\"(\\.jpg)|(\\.jpeg)|(\\.gif)|(\\.png)\", post))/50\n",
    "        for post in personality_data[\"posts\"]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 5;\n                var nbb_unformatted_code = \"def prep_data(personality_data):\\n\\n    t = time.time()\\n\\n    categorize_types(personality_data)\\n\\n    clean_posts(personality_data)\\n\\n    sentiment_score(personality_data)\\n\\n    pos_tagging(personality_data)\\n\\n    get_counts(personality_data)\\n\\n    features = personality_data[\\n        [\\n            \\\"clean_posts\\\",\\n            \\\"compound_sentiment\\\",\\n            \\\"ADJ_avg\\\",\\n            \\\"ADP_avg\\\",\\n            \\\"ADV_avg\\\",\\n            \\\"CONJ_avg\\\",\\n            \\\"DET_avg\\\",\\n            \\\"NOUN_avg\\\",\\n            \\\"NUM_avg\\\",\\n            \\\"PRT_avg\\\",\\n            \\\"PRON_avg\\\",\\n            \\\"VERB_avg\\\",\\n            \\\"qm\\\",\\n            \\\"em\\\",\\n            \\\"colons\\\",\\n            \\\"emojis\\\",\\n            \\\"word_count\\\",\\n            \\\"unique_words\\\",\\n            \\\"upper\\\",\\n            \\\"link_count\\\",\\n            \\\"ellipses\\\",\\n            \\\"img_count\\\",\\n        ]\\n    ]\\n\\n    X = features\\n    y = personality_data.iloc[:, 2:6]\\n\\n    print(f\\\"Total Preprocessing Time: {time.time()-t} seconds\\\\n\\\")\\n\\n    return X, y\";\n                var nbb_formatted_code = \"def prep_data(personality_data):\\n\\n    t = time.time()\\n\\n    categorize_types(personality_data)\\n\\n    clean_posts(personality_data)\\n\\n    sentiment_score(personality_data)\\n\\n    pos_tagging(personality_data)\\n\\n    get_counts(personality_data)\\n\\n    features = personality_data[\\n        [\\n            \\\"clean_posts\\\",\\n            \\\"compound_sentiment\\\",\\n            \\\"ADJ_avg\\\",\\n            \\\"ADP_avg\\\",\\n            \\\"ADV_avg\\\",\\n            \\\"CONJ_avg\\\",\\n            \\\"DET_avg\\\",\\n            \\\"NOUN_avg\\\",\\n            \\\"NUM_avg\\\",\\n            \\\"PRT_avg\\\",\\n            \\\"PRON_avg\\\",\\n            \\\"VERB_avg\\\",\\n            \\\"qm\\\",\\n            \\\"em\\\",\\n            \\\"colons\\\",\\n            \\\"emojis\\\",\\n            \\\"word_count\\\",\\n            \\\"unique_words\\\",\\n            \\\"upper\\\",\\n            \\\"link_count\\\",\\n            \\\"ellipses\\\",\\n            \\\"img_count\\\",\\n        ]\\n    ]\\n\\n    X = features\\n    y = personality_data.iloc[:, 2:6]\\n\\n    print(f\\\"Total Preprocessing Time: {time.time()-t} seconds\\\\n\\\")\\n\\n    return X, y\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prep_data(personality_data):\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    categorize_types(personality_data)\n",
    "\n",
    "    clean_posts(personality_data)\n",
    "\n",
    "    sentiment_score(personality_data)\n",
    "\n",
    "    pos_tagging(personality_data)\n",
    "\n",
    "    get_counts(personality_data)\n",
    "\n",
    "    features = personality_data[\n",
    "        [\n",
    "            \"clean_posts\",\n",
    "            \"compound_sentiment\",\n",
    "            \"ADJ_avg\",\n",
    "            \"ADP_avg\",\n",
    "            \"ADV_avg\",\n",
    "            \"CONJ_avg\",\n",
    "            \"DET_avg\",\n",
    "            \"NOUN_avg\",\n",
    "            \"NUM_avg\",\n",
    "            \"PRT_avg\",\n",
    "            \"PRON_avg\",\n",
    "            \"VERB_avg\",\n",
    "            \"qm\",\n",
    "            \"em\",\n",
    "            \"colons\",\n",
    "            \"emojis\",\n",
    "            \"word_count\",\n",
    "            \"unique_words\",\n",
    "            \"upper\",\n",
    "            \"link_count\",\n",
    "            \"ellipses\",\n",
    "            \"img_count\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    X = features\n",
    "    y = personality_data.iloc[:, 2:6]\n",
    "\n",
    "    print(f\"Total Preprocessing Time: {time.time()-t} seconds\\n\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 6;\n                var nbb_unformatted_code = \"def combine_classes(y_pred1, y_pred2, y_pred3, y_pred4):\\n    \\n    combined = []\\n    for i in range(len(y_pred1)):\\n        combined.append(\\n            str(y_pred1[i]) + str(y_pred2[i]) + str(y_pred3[i]) + str(y_pred4[i])\\n        )\\n    \\n    result = trace_back(combined)\\n    return result\\n    \\n\\ndef trace_back(combined):\\n        \\n    type_list = [\\n    {\\\"0\\\": \\\"I\\\", \\\"1\\\": \\\"E\\\"},\\n    {\\\"0\\\": \\\"N\\\", \\\"1\\\": \\\"S\\\"},\\n    {\\\"0\\\": \\\"F\\\", \\\"1\\\": \\\"T\\\"},\\n    {\\\"0\\\": \\\"P\\\", \\\"1\\\": \\\"J\\\"},\\n    ]\\n\\n    result = []\\n    for num in combined:\\n        s = \\\"\\\"\\n        for i in range(len(num)):\\n            s += type_list[i][num[i]]\\n        result.append(s)\\n        \\n    return result\";\n                var nbb_formatted_code = \"def combine_classes(y_pred1, y_pred2, y_pred3, y_pred4):\\n\\n    combined = []\\n    for i in range(len(y_pred1)):\\n        combined.append(\\n            str(y_pred1[i]) + str(y_pred2[i]) + str(y_pred3[i]) + str(y_pred4[i])\\n        )\\n\\n    result = trace_back(combined)\\n    return result\\n\\n\\ndef trace_back(combined):\\n\\n    type_list = [\\n        {\\\"0\\\": \\\"I\\\", \\\"1\\\": \\\"E\\\"},\\n        {\\\"0\\\": \\\"N\\\", \\\"1\\\": \\\"S\\\"},\\n        {\\\"0\\\": \\\"F\\\", \\\"1\\\": \\\"T\\\"},\\n        {\\\"0\\\": \\\"P\\\", \\\"1\\\": \\\"J\\\"},\\n    ]\\n\\n    result = []\\n    for num in combined:\\n        s = \\\"\\\"\\n        for i in range(len(num)):\\n            s += type_list[i][num[i]]\\n        result.append(s)\\n\\n    return result\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def combine_classes(y_pred1, y_pred2, y_pred3, y_pred4):\n",
    "    \n",
    "    combined = []\n",
    "    for i in range(len(y_pred1)):\n",
    "        combined.append(\n",
    "            str(y_pred1[i]) + str(y_pred2[i]) + str(y_pred3[i]) + str(y_pred4[i])\n",
    "        )\n",
    "    \n",
    "    result = trace_back(combined)\n",
    "    return result\n",
    "    \n",
    "\n",
    "def trace_back(combined):\n",
    "        \n",
    "    type_list = [\n",
    "    {\"0\": \"I\", \"1\": \"E\"},\n",
    "    {\"0\": \"N\", \"1\": \"S\"},\n",
    "    {\"0\": \"F\", \"1\": \"T\"},\n",
    "    {\"0\": \"P\", \"1\": \"J\"},\n",
    "    ]\n",
    "\n",
    "    result = []\n",
    "    for num in combined:\n",
    "        s = \"\"\n",
    "        for i in range(len(num)):\n",
    "            s += type_list[i][num[i]]\n",
    "        result.append(s)\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 7;\n                var nbb_unformatted_code = \"def predict(path_to_csv):\\n\\n    df = pd.read_csv(path_to_csv)\\n\\n    X, y = prep_data(df)\\n\\n    # loading the 4 models\\n    EorI_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Extrovert.joblib\\\"))\\n    SorN_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Sensing.joblib\\\"))\\n    TorF_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Thinking.joblib\\\"))\\n    JorP_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Judging.joblib\\\"))\\n\\n    # predicting\\n    EorI_pred = EorI_model.predict(X)\\n    print(\\n        \\\"Extrovert vs Introvert Accuracy: \\\",\\n        accuracy_score(y[\\\"is_Extrovert\\\"], EorI_pred),\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Extrovert\\\"].values)\\n    print(\\\"preds\\\", EorI_pred)\\n\\n    SorN_pred = SorN_model.predict(X)\\n    print(\\n        \\\"\\\\nSensing vs Intuition Accuracy: \\\", accuracy_score(y[\\\"is_Sensing\\\"], SorN_pred)\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Sensing\\\"].values)\\n    print(\\\"preds\\\", SorN_pred)\\n\\n    TorF_pred = TorF_model.predict(X)\\n    print(\\n        \\\"\\\\nThinking vs Feeling Accuracy: \\\", accuracy_score(y[\\\"is_Thinking\\\"], TorF_pred)\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Thinking\\\"].values)\\n    print(\\\"preds\\\", TorF_pred)\\n\\n    JorP_pred = JorP_model.predict(X)\\n    print(\\n        \\\"\\\\nJudging vs Perceiving Accuracy: \\\", accuracy_score(y[\\\"is_Judging\\\"], JorP_pred)\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Judging\\\"].values)\\n    print(\\\"preds\\\", JorP_pred)\\n\\n    # combining the predictions from the 4 models\\n    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\\n\\n    return result\";\n                var nbb_formatted_code = \"def predict(path_to_csv):\\n\\n    df = pd.read_csv(path_to_csv)\\n\\n    X, y = prep_data(df)\\n\\n    # loading the 4 models\\n    EorI_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Extrovert.joblib\\\"))\\n    SorN_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Sensing.joblib\\\"))\\n    TorF_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Thinking.joblib\\\"))\\n    JorP_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Judging.joblib\\\"))\\n\\n    # predicting\\n    EorI_pred = EorI_model.predict(X)\\n    print(\\n        \\\"Extrovert vs Introvert Accuracy: \\\",\\n        accuracy_score(y[\\\"is_Extrovert\\\"], EorI_pred),\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Extrovert\\\"].values)\\n    print(\\\"preds\\\", EorI_pred)\\n\\n    SorN_pred = SorN_model.predict(X)\\n    print(\\n        \\\"\\\\nSensing vs Intuition Accuracy: \\\", accuracy_score(y[\\\"is_Sensing\\\"], SorN_pred)\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Sensing\\\"].values)\\n    print(\\\"preds\\\", SorN_pred)\\n\\n    TorF_pred = TorF_model.predict(X)\\n    print(\\n        \\\"\\\\nThinking vs Feeling Accuracy: \\\", accuracy_score(y[\\\"is_Thinking\\\"], TorF_pred)\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Thinking\\\"].values)\\n    print(\\\"preds\\\", TorF_pred)\\n\\n    JorP_pred = JorP_model.predict(X)\\n    print(\\n        \\\"\\\\nJudging vs Perceiving Accuracy: \\\", accuracy_score(y[\\\"is_Judging\\\"], JorP_pred)\\n    )\\n    print(\\\"y_true\\\", y[\\\"is_Judging\\\"].values)\\n    print(\\\"preds\\\", JorP_pred)\\n\\n    # combining the predictions from the 4 models\\n    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\\n\\n    return result\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(path_to_csv):\n",
    "\n",
    "    df = pd.read_csv(path_to_csv)\n",
    "\n",
    "    X, y = prep_data(df)\n",
    "\n",
    "    # loading the 4 models\n",
    "    EorI_model = load(os.path.join(\"..\", \"models\", \"clf_is_Extrovert.joblib\"))\n",
    "    SorN_model = load(os.path.join(\"..\", \"models\", \"clf_is_Sensing.joblib\"))\n",
    "    TorF_model = load(os.path.join(\"..\", \"models\", \"clf_is_Thinking.joblib\"))\n",
    "    JorP_model = load(os.path.join(\"..\", \"models\", \"clf_is_Judging.joblib\"))\n",
    "\n",
    "    # predicting\n",
    "    EorI_pred = EorI_model.predict(X)\n",
    "    print(\n",
    "        \"Extrovert vs Introvert Accuracy: \",\n",
    "        accuracy_score(y[\"is_Extrovert\"], EorI_pred),\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Extrovert\"].values)\n",
    "    print(\"preds\", EorI_pred)\n",
    "\n",
    "    SorN_pred = SorN_model.predict(X)\n",
    "    print(\n",
    "        \"\\nSensing vs Intuition Accuracy: \", accuracy_score(y[\"is_Sensing\"], SorN_pred)\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Sensing\"].values)\n",
    "    print(\"preds\", SorN_pred)\n",
    "\n",
    "    TorF_pred = TorF_model.predict(X)\n",
    "    print(\n",
    "        \"\\nThinking vs Feeling Accuracy: \", accuracy_score(y[\"is_Thinking\"], TorF_pred)\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Thinking\"].values)\n",
    "    print(\"preds\", TorF_pred)\n",
    "\n",
    "    JorP_pred = JorP_model.predict(X)\n",
    "    print(\n",
    "        \"\\nJudging vs Perceiving Accuracy: \", accuracy_score(y[\"is_Judging\"], JorP_pred)\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Judging\"].values)\n",
    "    print(\"preds\", JorP_pred)\n",
    "\n",
    "    # combining the predictions from the 4 models\n",
    "    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Preprocessing Time: 83.77762842178345 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.0 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.0 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.23.0 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator SelectKBest from version 0.23.0 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator Pipeline from version 0.23.0 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator ColumnTransformer from version 0.23.0 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator LogisticRegressionCV from version 0.23.0 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ColumnTransformer' object has no attribute '_name_to_fitted_passthrough'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3ad4e98269a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"..\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"df_holdout.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0my_truth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"..\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"df_holdout.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"type\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-e4ecc3cd3c7d>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(path_to_csv)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# predicting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mEorI_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEorI_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     print(\n\u001b[0;32m     16\u001b[0m         \u001b[1;34m\"Extrovert vs Introvert Accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 799\u001b[1;33m         Xs = self._fit_transform(\n\u001b[0m\u001b[0;32m    800\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[1;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfitted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mensures\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0mare\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m         \"\"\"\n\u001b[1;32m--> 651\u001b[1;33m         transformers = list(\n\u001b[0m\u001b[0;32m    652\u001b[0m             self._iter(\n\u001b[0;32m    653\u001b[0m                 \u001b[0mfitted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfitted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace_strings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_as_strings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumn_as_strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36m_iter\u001b[1;34m(self, fitted, replace_strings, column_as_strings)\u001b[0m\n\u001b[0;32m    346\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_to_fitted_passthrough\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m                 transformers = [\n\u001b[0m\u001b[0;32m    349\u001b[0m                     \u001b[0mreplace_passthrough\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrans\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrans\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformers_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 ]\n",
      "\u001b[1;32mc:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 transformers = [\n\u001b[1;32m--> 349\u001b[1;33m                     \u001b[0mreplace_passthrough\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrans\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrans\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformers_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m                 ]\n\u001b[0;32m    351\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Huawei D15\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mreplace_passthrough\u001b[1;34m(name, trans, columns)\u001b[0m\n\u001b[0;32m    342\u001b[0m                 \u001b[1;31m# _name_to_fitted_passthrough\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m                 \u001b[1;32mdef\u001b[0m \u001b[0mreplace_passthrough\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feature_names_in\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_to_fitted_passthrough\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ColumnTransformer' object has no attribute '_name_to_fitted_passthrough'"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 12;\n                var nbb_unformatted_code = \"if __name__ == \\\"__main__\\\":\\n    \\n    predictions = predict(os.path.join(\\\"..\\\", \\\"data\\\", \\\"df_holdout.csv\\\"))\\n    y_truth = pd.read_csv(os.path.join(\\\"..\\\", \\\"data\\\", \\\"df_holdout.csv\\\"))[\\\"type\\\"].values\";\n                var nbb_formatted_code = \"if __name__ == \\\"__main__\\\":\\n\\n    predictions = predict(os.path.join(\\\"..\\\", \\\"data\\\", \\\"df_holdout.csv\\\"))\\n    y_truth = pd.read_csv(os.path.join(\\\"..\\\", \\\"data\\\", \\\"df_holdout.csv\\\"))[\\\"type\\\"].values\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    predictions = predict(os.path.join(\"..\", \"data\", \"df_holdout.csv\"))\n",
    "    y_truth = pd.read_csv(os.path.join(\"..\", \"data\", \"df_holdout.csv\"))[\"type\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m predictions\n\u001b[0;32m      2\u001b[0m df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "df[\"result\"] = predictions\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting for a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That somehow managed to be record short yet answer almost all the questions we would've asked, haha! Hi Deb! Welcome to Hou Tian; nice to meet you! I'm Jhenne, one of the mods here-- which means I gotta give you the modly speech :] Make sure to check out the Mandatory Reading up top! Our constantly updated Library is also a great resource, though it isn't mandatory reading-- we like to tell members to 'read as you need', rather than marathon read it all at once. One of the most helpful threads is the Gameplay So Far thread, which breaks down what all has gone down on the boards. (Now, the summary for January isn't tossed up yet, but we'd be happy to break down what you missed if you'd like.) I see that you're interested in Mai! That means both the Trying for a Canon Character page, and the Canon Character Rules and Consequences post will be helpful to check out. If you're ever considering an original character, we have our player-made adoptables list, and our factions, comprised of the Jade Shark/Bending Opposition, Original People of the Flame, and The Bending Crime Syndicates. As far as characters go, in the past tense I play Srai, a Jade Shark [s]that is very very dusty. In the Korraverse I play a reporter named Chihiro, and an ex-taxi dancer/wannabe actress named Naoki, and a Republic City University student named Haruna. I think that's it! If you have any questions, don't hesitate to ask a mod, or drop it right here in this thread so we can get back to you! Again, welcome! #CONFETTI\n",
      "                                         clean_posts  compound_sentiment  \\\n",
      "0  hat somehow managed record short yet answer al...              0.9834   \n",
      "\n",
      "   ADJ_avg  ADP_avg  ADV_avg  CONJ_avg  DET_avg  NOUN_avg  NUM_avg  PRT_avg  \\\n",
      "0       27       10       19        29       30        66        1        6   \n",
      "\n",
      "   ...  qm  em  colons  emojis  word_count  unique_words  upper  link_count  \\\n",
      "0  ...   0   8       1       0         271           180      6           0   \n",
      "\n",
      "   ellipses  img_count  \n",
      "0         0          0  \n",
      "\n",
      "[1 rows x 22 columns]\n",
      "Preprocessing Time: 0.33211350440979004 seconds\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 10;\n                var nbb_unformatted_code = \"mbti = [\\n    \\\"INFP\\\",\\n    \\\"INFJ\\\",\\n    \\\"INTP\\\",\\n    \\\"INTJ\\\",\\n    \\\"ENTP\\\",\\n    \\\"enfp\\\",\\n    \\\"ISTP\\\",\\n    \\\"ISFP\\\",\\n    \\\"ENTJ\\\",\\n    \\\"ISTJ\\\",\\n    \\\"ENFJ\\\",\\n    \\\"ISFJ\\\",\\n    \\\"ESTP\\\",\\n    \\\"ESFP\\\",\\n    \\\"ESFJ\\\",\\n    \\\"ESTJ\\\",\\n]\\ntags_dict = {\\n    \\\"ADJ_avg\\\": [\\\"JJ\\\", \\\"JJR\\\", \\\"JJS\\\"],\\n    \\\"ADP_avg\\\": [\\\"EX\\\", \\\"TO\\\"],\\n    \\\"ADV_avg\\\": [\\\"RB\\\", \\\"RBR\\\", \\\"RBS\\\", \\\"WRB\\\"],\\n    \\\"CONJ_avg\\\": [\\\"CC\\\", \\\"IN\\\"],\\n    \\\"DET_avg\\\": [\\\"DT\\\", \\\"PDT\\\", \\\"WDT\\\"],\\n    \\\"NOUN_avg\\\": [\\\"NN\\\", \\\"NNS\\\", \\\"NNP\\\", \\\"NNPS\\\"],\\n    \\\"NUM_avg\\\": [\\\"CD\\\"],\\n    \\\"PRT_avg\\\": [\\\"RP\\\"],\\n    \\\"PRON_avg\\\": [\\\"PRP\\\", \\\"PRP$\\\", \\\"WP\\\", \\\"WP$\\\"],\\n    \\\"VERB_avg\\\": [\\\"MD\\\", \\\"VB\\\", \\\"VBD\\\", \\\"VBG\\\", \\\"VBN\\\", \\\"VBP\\\", \\\"VBZ\\\"],\\n    \\\".\\\": [\\\"#\\\", \\\"$\\\", \\\"''\\\", \\\"(\\\", \\\")\\\", \\\",\\\", \\\".\\\", \\\":\\\"],\\n    \\\"X\\\": [\\\"FW\\\", \\\"LS\\\", \\\"UH\\\"],\\n}\\nfeatures = [\\n    \\\"clean_posts\\\",\\n    \\\"compound_sentiment\\\",\\n    \\\"ADJ_avg\\\",\\n    \\\"ADP_avg\\\",\\n    \\\"ADV_avg\\\",\\n    \\\"CONJ_avg\\\",\\n    \\\"DET_avg\\\",\\n    \\\"NOUN_avg\\\",\\n    \\\"NUM_avg\\\",\\n    \\\"PRT_avg\\\",\\n    \\\"PRON_avg\\\",\\n    \\\"VERB_avg\\\",\\n    \\\"qm\\\",\\n    \\\"em\\\",\\n    \\\"colons\\\",\\n    \\\"emojis\\\",\\n    \\\"word_count\\\",\\n    \\\"unique_words\\\",\\n    \\\"upper\\\",\\n    \\\"link_count\\\",\\n    \\\"ellipses\\\",\\n    \\\"img_count\\\",\\n]\\n\\n\\ndef unique_words(s):\\n    unique = set(s.split(\\\" \\\"))\\n    return len(unique)\\n\\n\\ndef emojis(post):\\n    # does not include emojis made purely from symbols, only :word:\\n    emoji_count = 0\\n    words = post.split()\\n    for e in words:\\n        if \\\"http\\\" not in e:\\n            if e.count(\\\":\\\") == 2:\\n                emoji_count += 1\\n    return emoji_count\\n\\n\\ndef colons(post):\\n    # Includes colons used in emojis\\n    colon_count = 0\\n    words = post.split()\\n    for e in words:\\n        if \\\"http\\\" not in e:\\n            colon_count += e.count(\\\":\\\")\\n    return colon_count\\n\\n\\ndef lemmitize(s):\\n    lemmatizer = WordNetLemmatizer()\\n    new_s = \\\"\\\"\\n    for word in s.split(\\\" \\\"):\\n        lemmatizer.lemmatize(word)\\n        if word not in stopwords.words(\\\"english\\\"):\\n            new_s += word + \\\" \\\"\\n    return new_s[:-1]\\n\\n\\ndef clean(s):\\n    # remove urls\\n    s = re.sub(re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+).*\\\"), \\\"\\\", s)\\n    # remove emails\\n    s = re.sub(re.compile(r\\\"\\\\S+@\\\\S+\\\"), \\\"\\\", s)\\n    # remove punctuation\\n    s = re.sub(re.compile(r\\\"[^a-z\\\\s]\\\"), \\\"\\\", s)\\n    # Make everything lowercase\\n    s = s.lower()\\n    # remove all personality types\\n    for type_word in mbti:\\n        s = s.replace(type_word.lower(), \\\"\\\")\\n    return s\\n\\n\\ndef prep_counts(s):\\n    clean_s = clean(s)\\n    d = {\\n        \\\"clean_posts\\\": lemmitize(clean_s),\\n        \\\"link_count\\\": s.count(\\\"http\\\"),\\n        \\\"youtube\\\": s.count(\\\"youtube\\\") + s.count(\\\"youtu.be\\\"),\\n        \\\"img_count\\\": len(re.findall(r\\\"(\\\\.jpg)|(\\\\.jpeg)|(\\\\.gif)|(\\\\.png)\\\", s)),\\n        \\\"upper\\\": len([x for x in s.split() if x.isupper()]),\\n        \\\"char_count\\\": len(s),\\n        \\\"word_count\\\": clean_s.count(\\\" \\\") + 1,\\n        \\\"qm\\\": s.count(\\\"?\\\"),\\n        \\\"em\\\": s.count(\\\"!\\\"),\\n        \\\"colons\\\": colons(s),\\n        \\\"emojis\\\": emojis(s),\\n        \\\"unique_words\\\": unique_words(clean_s),\\n        \\\"ellipses\\\": len(re.findall(r\\\"\\\\.\\\\.\\\\.\\\\ \\\", s)),\\n    }\\n    return clean_s, d\\n\\n\\ndef prep_sentiment(s):\\n    analyzer = SentimentIntensityAnalyzer()\\n    score = analyzer.polarity_scores(s)\\n    d = {\\n        \\\"compound_sentiment\\\": score[\\\"compound\\\"],\\n        \\\"pos_sentiment\\\": score[\\\"pos\\\"],\\n        \\\"neg_sentiment\\\": score[\\\"neg\\\"],\\n        \\\"neu_sentiment\\\": score[\\\"neu\\\"],\\n    }\\n    return d\\n\\n\\ndef tag_pos(s):\\n    tagged_words = nltk.pos_tag(word_tokenize(s))\\n    d = dict.fromkeys(tags_dict, 0)\\n    for tup in tagged_words:\\n        tag = tup[1]\\n        for key, val in tags_dict.items():\\n            if tag in val:\\n                tag = key\\n        d[tag] += 1\\n    return d\\n\\n\\ndef prep_data(s):\\n    clean_s, d = prep_counts(s)\\n    d.update(prep_sentiment(lemmitize(clean_s)))\\n    d.update(tag_pos(clean_s))\\n    return pd.DataFrame([d])[features]\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    t = time.time()\\n    string = \\\"That somehow managed to be record short yet answer almost all the questions we would've asked, haha! Hi Deb! Welcome to Hou Tian; nice to meet you! I'm Jhenne, one of the mods here-- which means I gotta give you the modly speech :] Make sure to check out the Mandatory Reading up top! Our constantly updated Library is also a great resource, though it isn't mandatory reading-- we like to tell members to 'read as you need', rather than marathon read it all at once. One of the most helpful threads is the Gameplay So Far thread, which breaks down what all has gone down on the boards. (Now, the summary for January isn't tossed up yet, but we'd be happy to break down what you missed if you'd like.) I see that you're interested in Mai! That means both the Trying for a Canon Character page, and the Canon Character Rules and Consequences post will be helpful to check out. If you're ever considering an original character, we have our player-made adoptables list, and our factions, comprised of the Jade Shark/Bending Opposition, Original People of the Flame, and The Bending Crime Syndicates. As far as characters go, in the past tense I play Srai, a Jade Shark [s]that is very very dusty. In the Korraverse I play a reporter named Chihiro, and an ex-taxi dancer/wannabe actress named Naoki, and a Republic City University student named Haruna. I think that's it! If you have any questions, don't hesitate to ask a mod, or drop it right here in this thread so we can get back to you! Again, welcome! #CONFETTI\\\"\\n    print(string)\\n    print(prep_data(string))\\n    print(f\\\"Preprocessing Time: {time.time() - t} seconds\\\")\";\n                var nbb_formatted_code = \"mbti = [\\n    \\\"INFP\\\",\\n    \\\"INFJ\\\",\\n    \\\"INTP\\\",\\n    \\\"INTJ\\\",\\n    \\\"ENTP\\\",\\n    \\\"enfp\\\",\\n    \\\"ISTP\\\",\\n    \\\"ISFP\\\",\\n    \\\"ENTJ\\\",\\n    \\\"ISTJ\\\",\\n    \\\"ENFJ\\\",\\n    \\\"ISFJ\\\",\\n    \\\"ESTP\\\",\\n    \\\"ESFP\\\",\\n    \\\"ESFJ\\\",\\n    \\\"ESTJ\\\",\\n]\\ntags_dict = {\\n    \\\"ADJ_avg\\\": [\\\"JJ\\\", \\\"JJR\\\", \\\"JJS\\\"],\\n    \\\"ADP_avg\\\": [\\\"EX\\\", \\\"TO\\\"],\\n    \\\"ADV_avg\\\": [\\\"RB\\\", \\\"RBR\\\", \\\"RBS\\\", \\\"WRB\\\"],\\n    \\\"CONJ_avg\\\": [\\\"CC\\\", \\\"IN\\\"],\\n    \\\"DET_avg\\\": [\\\"DT\\\", \\\"PDT\\\", \\\"WDT\\\"],\\n    \\\"NOUN_avg\\\": [\\\"NN\\\", \\\"NNS\\\", \\\"NNP\\\", \\\"NNPS\\\"],\\n    \\\"NUM_avg\\\": [\\\"CD\\\"],\\n    \\\"PRT_avg\\\": [\\\"RP\\\"],\\n    \\\"PRON_avg\\\": [\\\"PRP\\\", \\\"PRP$\\\", \\\"WP\\\", \\\"WP$\\\"],\\n    \\\"VERB_avg\\\": [\\\"MD\\\", \\\"VB\\\", \\\"VBD\\\", \\\"VBG\\\", \\\"VBN\\\", \\\"VBP\\\", \\\"VBZ\\\"],\\n    \\\".\\\": [\\\"#\\\", \\\"$\\\", \\\"''\\\", \\\"(\\\", \\\")\\\", \\\",\\\", \\\".\\\", \\\":\\\"],\\n    \\\"X\\\": [\\\"FW\\\", \\\"LS\\\", \\\"UH\\\"],\\n}\\nfeatures = [\\n    \\\"clean_posts\\\",\\n    \\\"compound_sentiment\\\",\\n    \\\"ADJ_avg\\\",\\n    \\\"ADP_avg\\\",\\n    \\\"ADV_avg\\\",\\n    \\\"CONJ_avg\\\",\\n    \\\"DET_avg\\\",\\n    \\\"NOUN_avg\\\",\\n    \\\"NUM_avg\\\",\\n    \\\"PRT_avg\\\",\\n    \\\"PRON_avg\\\",\\n    \\\"VERB_avg\\\",\\n    \\\"qm\\\",\\n    \\\"em\\\",\\n    \\\"colons\\\",\\n    \\\"emojis\\\",\\n    \\\"word_count\\\",\\n    \\\"unique_words\\\",\\n    \\\"upper\\\",\\n    \\\"link_count\\\",\\n    \\\"ellipses\\\",\\n    \\\"img_count\\\",\\n]\\n\\n\\ndef unique_words(s):\\n    unique = set(s.split(\\\" \\\"))\\n    return len(unique)\\n\\n\\ndef emojis(post):\\n    # does not include emojis made purely from symbols, only :word:\\n    emoji_count = 0\\n    words = post.split()\\n    for e in words:\\n        if \\\"http\\\" not in e:\\n            if e.count(\\\":\\\") == 2:\\n                emoji_count += 1\\n    return emoji_count\\n\\n\\ndef colons(post):\\n    # Includes colons used in emojis\\n    colon_count = 0\\n    words = post.split()\\n    for e in words:\\n        if \\\"http\\\" not in e:\\n            colon_count += e.count(\\\":\\\")\\n    return colon_count\\n\\n\\ndef lemmitize(s):\\n    lemmatizer = WordNetLemmatizer()\\n    new_s = \\\"\\\"\\n    for word in s.split(\\\" \\\"):\\n        lemmatizer.lemmatize(word)\\n        if word not in stopwords.words(\\\"english\\\"):\\n            new_s += word + \\\" \\\"\\n    return new_s[:-1]\\n\\n\\ndef clean(s):\\n    # remove urls\\n    s = re.sub(re.compile(r\\\"https?:\\\\/\\\\/(www)?.?([A-Za-z_0-9-]+).*\\\"), \\\"\\\", s)\\n    # remove emails\\n    s = re.sub(re.compile(r\\\"\\\\S+@\\\\S+\\\"), \\\"\\\", s)\\n    # remove punctuation\\n    s = re.sub(re.compile(r\\\"[^a-z\\\\s]\\\"), \\\"\\\", s)\\n    # Make everything lowercase\\n    s = s.lower()\\n    # remove all personality types\\n    for type_word in mbti:\\n        s = s.replace(type_word.lower(), \\\"\\\")\\n    return s\\n\\n\\ndef prep_counts(s):\\n    clean_s = clean(s)\\n    d = {\\n        \\\"clean_posts\\\": lemmitize(clean_s),\\n        \\\"link_count\\\": s.count(\\\"http\\\"),\\n        \\\"youtube\\\": s.count(\\\"youtube\\\") + s.count(\\\"youtu.be\\\"),\\n        \\\"img_count\\\": len(re.findall(r\\\"(\\\\.jpg)|(\\\\.jpeg)|(\\\\.gif)|(\\\\.png)\\\", s)),\\n        \\\"upper\\\": len([x for x in s.split() if x.isupper()]),\\n        \\\"char_count\\\": len(s),\\n        \\\"word_count\\\": clean_s.count(\\\" \\\") + 1,\\n        \\\"qm\\\": s.count(\\\"?\\\"),\\n        \\\"em\\\": s.count(\\\"!\\\"),\\n        \\\"colons\\\": colons(s),\\n        \\\"emojis\\\": emojis(s),\\n        \\\"unique_words\\\": unique_words(clean_s),\\n        \\\"ellipses\\\": len(re.findall(r\\\"\\\\.\\\\.\\\\.\\\\ \\\", s)),\\n    }\\n    return clean_s, d\\n\\n\\ndef prep_sentiment(s):\\n    analyzer = SentimentIntensityAnalyzer()\\n    score = analyzer.polarity_scores(s)\\n    d = {\\n        \\\"compound_sentiment\\\": score[\\\"compound\\\"],\\n        \\\"pos_sentiment\\\": score[\\\"pos\\\"],\\n        \\\"neg_sentiment\\\": score[\\\"neg\\\"],\\n        \\\"neu_sentiment\\\": score[\\\"neu\\\"],\\n    }\\n    return d\\n\\n\\ndef tag_pos(s):\\n    tagged_words = nltk.pos_tag(word_tokenize(s))\\n    d = dict.fromkeys(tags_dict, 0)\\n    for tup in tagged_words:\\n        tag = tup[1]\\n        for key, val in tags_dict.items():\\n            if tag in val:\\n                tag = key\\n        d[tag] += 1\\n    return d\\n\\n\\ndef prep_data(s):\\n    clean_s, d = prep_counts(s)\\n    d.update(prep_sentiment(lemmitize(clean_s)))\\n    d.update(tag_pos(clean_s))\\n    return pd.DataFrame([d])[features]\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    t = time.time()\\n    string = \\\"That somehow managed to be record short yet answer almost all the questions we would've asked, haha! Hi Deb! Welcome to Hou Tian; nice to meet you! I'm Jhenne, one of the mods here-- which means I gotta give you the modly speech :] Make sure to check out the Mandatory Reading up top! Our constantly updated Library is also a great resource, though it isn't mandatory reading-- we like to tell members to 'read as you need', rather than marathon read it all at once. One of the most helpful threads is the Gameplay So Far thread, which breaks down what all has gone down on the boards. (Now, the summary for January isn't tossed up yet, but we'd be happy to break down what you missed if you'd like.) I see that you're interested in Mai! That means both the Trying for a Canon Character page, and the Canon Character Rules and Consequences post will be helpful to check out. If you're ever considering an original character, we have our player-made adoptables list, and our factions, comprised of the Jade Shark/Bending Opposition, Original People of the Flame, and The Bending Crime Syndicates. As far as characters go, in the past tense I play Srai, a Jade Shark [s]that is very very dusty. In the Korraverse I play a reporter named Chihiro, and an ex-taxi dancer/wannabe actress named Naoki, and a Republic City University student named Haruna. I think that's it! If you have any questions, don't hesitate to ask a mod, or drop it right here in this thread so we can get back to you! Again, welcome! #CONFETTI\\\"\\n    print(string)\\n    print(prep_data(string))\\n    print(f\\\"Preprocessing Time: {time.time() - t} seconds\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mbti = [\n",
    "    \"INFP\",\n",
    "    \"INFJ\",\n",
    "    \"INTP\",\n",
    "    \"INTJ\",\n",
    "    \"ENTP\",\n",
    "    \"enfp\",\n",
    "    \"ISTP\",\n",
    "    \"ISFP\",\n",
    "    \"ENTJ\",\n",
    "    \"ISTJ\",\n",
    "    \"ENFJ\",\n",
    "    \"ISFJ\",\n",
    "    \"ESTP\",\n",
    "    \"ESFP\",\n",
    "    \"ESFJ\",\n",
    "    \"ESTJ\",\n",
    "]\n",
    "tags_dict = {\n",
    "    \"ADJ_avg\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "    \"ADP_avg\": [\"EX\", \"TO\"],\n",
    "    \"ADV_avg\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "    \"CONJ_avg\": [\"CC\", \"IN\"],\n",
    "    \"DET_avg\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "    \"NOUN_avg\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "    \"NUM_avg\": [\"CD\"],\n",
    "    \"PRT_avg\": [\"RP\"],\n",
    "    \"PRON_avg\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "    \"VERB_avg\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "    \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "    \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "}\n",
    "features = [\n",
    "    \"clean_posts\",\n",
    "    \"compound_sentiment\",\n",
    "    \"ADJ_avg\",\n",
    "    \"ADP_avg\",\n",
    "    \"ADV_avg\",\n",
    "    \"CONJ_avg\",\n",
    "    \"DET_avg\",\n",
    "    \"NOUN_avg\",\n",
    "    \"NUM_avg\",\n",
    "    \"PRT_avg\",\n",
    "    \"PRON_avg\",\n",
    "    \"VERB_avg\",\n",
    "    \"qm\",\n",
    "    \"em\",\n",
    "    \"colons\",\n",
    "    \"emojis\",\n",
    "    \"word_count\",\n",
    "    \"unique_words\",\n",
    "    \"upper\",\n",
    "    \"link_count\",\n",
    "    \"ellipses\",\n",
    "    \"img_count\",\n",
    "]\n",
    "\n",
    "\n",
    "def unique_words(s):\n",
    "    unique = set(s.split(\" \"))\n",
    "    return len(unique)\n",
    "\n",
    "\n",
    "def emojis(post):\n",
    "    # does not include emojis made purely from symbols, only :word:\n",
    "    emoji_count = 0\n",
    "    words = post.split()\n",
    "    for e in words:\n",
    "        if \"http\" not in e:\n",
    "            if e.count(\":\") == 2:\n",
    "                emoji_count += 1\n",
    "    return emoji_count\n",
    "\n",
    "\n",
    "def colons(post):\n",
    "    # Includes colons used in emojis\n",
    "    colon_count = 0\n",
    "    words = post.split()\n",
    "    for e in words:\n",
    "        if \"http\" not in e:\n",
    "            colon_count += e.count(\":\")\n",
    "    return colon_count\n",
    "\n",
    "\n",
    "def lemmitize(s):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_s = \"\"\n",
    "    for word in s.split(\" \"):\n",
    "        lemmatizer.lemmatize(word)\n",
    "        if word not in stopwords.words(\"english\"):\n",
    "            new_s += word + \" \"\n",
    "    return new_s[:-1]\n",
    "\n",
    "\n",
    "def clean(s):\n",
    "    # remove urls\n",
    "    s = re.sub(re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+).*\"), \"\", s)\n",
    "    # remove emails\n",
    "    s = re.sub(re.compile(r\"\\S+@\\S+\"), \"\", s)\n",
    "    # remove punctuation\n",
    "    s = re.sub(re.compile(r\"[^a-z\\s]\"), \"\", s)\n",
    "    # Make everything lowercase\n",
    "    s = s.lower()\n",
    "    # remove all personality types\n",
    "    for type_word in mbti:\n",
    "        s = s.replace(type_word.lower(), \"\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def prep_counts(s):\n",
    "    clean_s = clean(s)\n",
    "    d = {\n",
    "        \"clean_posts\": lemmitize(clean_s),\n",
    "        \"link_count\": s.count(\"http\"),\n",
    "        \"youtube\": s.count(\"youtube\") + s.count(\"youtu.be\"),\n",
    "        \"img_count\": len(re.findall(r\"(\\.jpg)|(\\.jpeg)|(\\.gif)|(\\.png)\", s)),\n",
    "        \"upper\": len([x for x in s.split() if x.isupper()]),\n",
    "        \"char_count\": len(s),\n",
    "        \"word_count\": clean_s.count(\" \") + 1,\n",
    "        \"qm\": s.count(\"?\"),\n",
    "        \"em\": s.count(\"!\"),\n",
    "        \"colons\": colons(s),\n",
    "        \"emojis\": emojis(s),\n",
    "        \"unique_words\": unique_words(clean_s),\n",
    "        \"ellipses\": len(re.findall(r\"\\.\\.\\.\\ \", s)),\n",
    "    }\n",
    "    return clean_s, d\n",
    "\n",
    "\n",
    "def prep_sentiment(s):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    score = analyzer.polarity_scores(s)\n",
    "    d = {\n",
    "        \"compound_sentiment\": score[\"compound\"],\n",
    "        \"pos_sentiment\": score[\"pos\"],\n",
    "        \"neg_sentiment\": score[\"neg\"],\n",
    "        \"neu_sentiment\": score[\"neu\"],\n",
    "    }\n",
    "    return d\n",
    "\n",
    "\n",
    "def tag_pos(s):\n",
    "    tagged_words = nltk.pos_tag(word_tokenize(s))\n",
    "    d = dict.fromkeys(tags_dict, 0)\n",
    "    for tup in tagged_words:\n",
    "        tag = tup[1]\n",
    "        for key, val in tags_dict.items():\n",
    "            if tag in val:\n",
    "                tag = key\n",
    "        d[tag] += 1\n",
    "    return d\n",
    "\n",
    "\n",
    "def prep_data(s):\n",
    "    clean_s, d = prep_counts(s)\n",
    "    d.update(prep_sentiment(lemmitize(clean_s)))\n",
    "    d.update(tag_pos(clean_s))\n",
    "    return pd.DataFrame([d])[features]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time()\n",
    "    string = \"That somehow managed to be record short yet answer almost all the questions we would've asked, haha! Hi Deb! Welcome to Hou Tian; nice to meet you! I'm Jhenne, one of the mods here-- which means I gotta give you the modly speech :] Make sure to check out the Mandatory Reading up top! Our constantly updated Library is also a great resource, though it isn't mandatory reading-- we like to tell members to 'read as you need', rather than marathon read it all at once. One of the most helpful threads is the Gameplay So Far thread, which breaks down what all has gone down on the boards. (Now, the summary for January isn't tossed up yet, but we'd be happy to break down what you missed if you'd like.) I see that you're interested in Mai! That means both the Trying for a Canon Character page, and the Canon Character Rules and Consequences post will be helpful to check out. If you're ever considering an original character, we have our player-made adoptables list, and our factions, comprised of the Jade Shark/Bending Opposition, Original People of the Flame, and The Bending Crime Syndicates. As far as characters go, in the past tense I play Srai, a Jade Shark [s]that is very very dusty. In the Korraverse I play a reporter named Chihiro, and an ex-taxi dancer/wannabe actress named Naoki, and a Republic City University student named Haruna. I think that's it! If you have any questions, don't hesitate to ask a mod, or drop it right here in this thread so we can get back to you! Again, welcome! #CONFETTI\"\n",
    "    print(string)\n",
    "    print(prep_data(string))\n",
    "    print(f\"Preprocessing Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds [0]\n",
      "preds [0]\n",
      "preds [1]\n",
      "preds [0]\n",
      "INTP\n",
      "Preprocessing Time: 2.15 seconds\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 11;\n                var nbb_unformatted_code = \"def predict(s):\\n    return len(s.split(\\\" \\\"))\\n\\n\\ndef predict_e(s):\\n\\n    X = prep_data(s)\\n\\n    # loading the 4 models\\n    EorI_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Extrovert.joblib\\\"))\\n    SorN_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Sensing.joblib\\\"))\\n    TorF_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Thinking.joblib\\\"))\\n    JorP_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Judging.joblib\\\"))\\n\\n    # predicting\\n    EorI_pred = EorI_model.predict(X)\\n    print(\\\"preds\\\", EorI_pred)\\n\\n    SorN_pred = SorN_model.predict(X)\\n    print(\\\"preds\\\", SorN_pred)\\n\\n    TorF_pred = TorF_model.predict(X)\\n    print(\\\"preds\\\", TorF_pred)\\n\\n    JorP_pred = JorP_model.predict(X)\\n    print(\\\"preds\\\", JorP_pred)\\n\\n    # combining the predictions from the 4 models\\n    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\\n\\n    return result[0]\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    t = time.time()\\n    # sample test string. Type ISTP.\\n    string = \\\"I plugged the data into tableau to see how the different features or how various mathematical formulas relate to the Weight. Once I had a few that didn\\u2019t have a wide distribution, I just started trying different models, even ones we hadn\\u2019t gone over yet. There are a LOT of regression models. I do not like this try everything method, it\\u2019s inefficient and illogical.\\\"\\n    print(predict_e(string))\\n    print(f\\\"Preprocessing Time: {(time.time() - t):.2f} seconds\\\")\";\n                var nbb_formatted_code = \"def predict(s):\\n    return len(s.split(\\\" \\\"))\\n\\n\\ndef predict_e(s):\\n\\n    X = prep_data(s)\\n\\n    # loading the 4 models\\n    EorI_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Extrovert.joblib\\\"))\\n    SorN_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Sensing.joblib\\\"))\\n    TorF_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Thinking.joblib\\\"))\\n    JorP_model = load(os.path.join(\\\"..\\\", \\\"models\\\", \\\"clf_is_Judging.joblib\\\"))\\n\\n    # predicting\\n    EorI_pred = EorI_model.predict(X)\\n    print(\\\"preds\\\", EorI_pred)\\n\\n    SorN_pred = SorN_model.predict(X)\\n    print(\\\"preds\\\", SorN_pred)\\n\\n    TorF_pred = TorF_model.predict(X)\\n    print(\\\"preds\\\", TorF_pred)\\n\\n    JorP_pred = JorP_model.predict(X)\\n    print(\\\"preds\\\", JorP_pred)\\n\\n    # combining the predictions from the 4 models\\n    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\\n\\n    return result[0]\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    t = time.time()\\n    # sample test string. Type ISTP.\\n    string = \\\"I plugged the data into tableau to see how the different features or how various mathematical formulas relate to the Weight. Once I had a few that didn\\u2019t have a wide distribution, I just started trying different models, even ones we hadn\\u2019t gone over yet. There are a LOT of regression models. I do not like this try everything method, it\\u2019s inefficient and illogical.\\\"\\n    print(predict_e(string))\\n    print(f\\\"Preprocessing Time: {(time.time() - t):.2f} seconds\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(s):\n",
    "    return len(s.split(\" \"))\n",
    "\n",
    "\n",
    "def predict_e(s):\n",
    "\n",
    "    X = prep_data(s)\n",
    "\n",
    "    # loading the 4 models\n",
    "    EorI_model = load(os.path.join(\"..\", \"models\", \"clf_is_Extrovert.joblib\"))\n",
    "    SorN_model = load(os.path.join(\"..\", \"models\", \"clf_is_Sensing.joblib\"))\n",
    "    TorF_model = load(os.path.join(\"..\", \"models\", \"clf_is_Thinking.joblib\"))\n",
    "    JorP_model = load(os.path.join(\"..\", \"models\", \"clf_is_Judging.joblib\"))\n",
    "\n",
    "    # predicting\n",
    "    EorI_pred = EorI_model.predict(X)\n",
    "    print(\"preds\", EorI_pred)\n",
    "\n",
    "    SorN_pred = SorN_model.predict(X)\n",
    "    print(\"preds\", SorN_pred)\n",
    "\n",
    "    TorF_pred = TorF_model.predict(X)\n",
    "    print(\"preds\", TorF_pred)\n",
    "\n",
    "    JorP_pred = JorP_model.predict(X)\n",
    "    print(\"preds\", JorP_pred)\n",
    "\n",
    "    # combining the predictions from the 4 models\n",
    "    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\n",
    "\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time()\n",
    "    # sample test string. Type ISTP.\n",
    "    string = \"I plugged the data into tableau to see how the different features or how various mathematical formulas relate to the Weight. Once I had a few that didnt have a wide distribution, I just started trying different models, even ones we hadnt gone over yet. There are a LOT of regression models. I do not like this try everything method, its inefficient and illogical.\"\n",
    "    print(predict_e(string))\n",
    "    print(f\"Preprocessing Time: {(time.time() - t):.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cdba3d1713347338e6fa1455c6c3086d5bdeaa151e7e53a03f3a0575feb8c6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
