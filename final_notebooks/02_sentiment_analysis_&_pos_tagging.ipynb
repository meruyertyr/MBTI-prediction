{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## SENTIMENT ANALYSIS & PART OF SPEECH TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"# importing dependencies here\\nimport numpy as np\\nimport pandas as pd\\nimport os\\n\\n# feature engineering\\nimport re\\n\\n# pos tagging\\nimport nltk\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\n\\n# sentiment scoring\\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\\n# scaling to handle negative values in sentiment scores (for Naive Bayes)\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# performance check\\nimport time\\n\\n# code formatter\\n%load_ext nb_black\";\n                var nbb_formatted_code = \"# importing dependencies here\\nimport numpy as np\\nimport pandas as pd\\nimport os\\n\\n# feature engineering\\nimport re\\n\\n# pos tagging\\nimport nltk\\nfrom nltk.tokenize import word_tokenize, sent_tokenize\\n\\n# sentiment scoring\\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\\n# scaling to handle negative values in sentiment scores (for Naive Bayes)\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# performance check\\nimport time\\n\\n# code formatter\\n%load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing dependencies here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# feature engineering\n",
    "import re\n",
    "\n",
    "# pos tagging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# sentiment scoring\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# scaling to handle negative values in sentiment scores (for Naive Bayes)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# performance check\n",
    "import time\n",
    "\n",
    "# code formatter\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 3;\n                var nbb_unformatted_code = \"# reading the clean_dataset_1\\npersonality_data = pd.read_csv(os.path.join(\\\"..\\\", \\\"data\\\", \\\"clean_data_1.csv\\\"))\";\n                var nbb_formatted_code = \"# reading the clean_dataset_1\\npersonality_data = pd.read_csv(os.path.join(\\\"..\\\", \\\"data\\\", \\\"clean_data_1.csv\\\"))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reading the clean_dataset_1\n",
    "personality_data = pd.read_csv(os.path.join(\"..\", \"data\", \"clean_data_1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>is_Extrovert</th>\n",
       "      <th>is_Sensing</th>\n",
       "      <th>is_Thinking</th>\n",
       "      <th>is_Judging</th>\n",
       "      <th>posts</th>\n",
       "      <th>clean_posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'When asked of the things you wish you did ear...</td>\n",
       "      <td>when asked  the things you wish you did earli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'I love both and they are equally important to...</td>\n",
       "      <td>love both and they are equally important    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Really? You think implying that everyone who i...</td>\n",
       "      <td>really  you think implying that everyone who i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'Love is a crazy thing.   Se is our best form ...</td>\n",
       "      <td>love   crazy thing      our best form  commun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INTP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'I am a physics undergrad with a computation e...</td>\n",
       "      <td>physics undergrad with  computation emphas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  is_Extrovert  is_Sensing  is_Thinking  is_Judging  \\\n",
       "0  INFJ             0           0            0           1   \n",
       "1  INFJ             0           0            0           1   \n",
       "2  INFJ             0           0            0           1   \n",
       "3  ENFJ             1           0            0           1   \n",
       "4  INTP             0           0            1           0   \n",
       "\n",
       "                                               posts  \\\n",
       "0  'When asked of the things you wish you did ear...   \n",
       "1  'I love both and they are equally important to...   \n",
       "2  Really? You think implying that everyone who i...   \n",
       "3  'Love is a crazy thing.   Se is our best form ...   \n",
       "4  'I am a physics undergrad with a computation e...   \n",
       "\n",
       "                                         clean_posts  \n",
       "0   when asked  the things you wish you did earli...  \n",
       "1    love both and they are equally important    ...  \n",
       "2  really  you think implying that everyone who i...  \n",
       "3   love   crazy thing      our best form  commun...  \n",
       "4      physics undergrad with  computation emphas...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the top 5 rows of the dataset\n",
    "personality_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8588, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the number of rows and columns\n",
    "personality_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type            0\n",
       "is_Extrovert    0\n",
       "is_Sensing      0\n",
       "is_Thinking     0\n",
       "is_Judging      0\n",
       "posts           0\n",
       "clean_posts     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "personality_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values present in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiments Analysis Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CAUTION - Sentiment scoring will take LONG !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Scoring Time: 604.15 seconds\n"
     ]
    }
   ],
   "source": [
    "# sentiment scoring for each user\n",
    "t = time.time()\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "nlp_sentiment_score = []\n",
    "\n",
    "for post in personality_data[\"clean_posts\"]:\n",
    "    score = analyzer.polarity_scores(post)\n",
    "    nlp_sentiment_score.append(score)\n",
    "\n",
    "print(f\"Sentiment Scoring Time: {time.time() - t:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segregating the indiviual sentiment scores - compound, positive, negative and neutral\n",
    "personality_data[\"compound_sentiment\"] = [\n",
    "    score[\"compound\"] for score in nlp_sentiment_score\n",
    "]\n",
    "personality_data[\"pos_sentiment\"] = [score[\"pos\"] for score in nlp_sentiment_score]\n",
    "personality_data[\"neg_sentiment\"] = [score[\"neg\"] for score in nlp_sentiment_score]\n",
    "personality_data[\"neu_sentiment\"] = [score[\"neu\"] for score in nlp_sentiment_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment scores have negative values that Naive Bayes can't handle. So scaling it.\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "personality_data[\"compound_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"compound_sentiment\"]).reshape(-1, 1)\n",
    ")\n",
    "personality_data[\"pos_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"pos_sentiment\"]).reshape(-1, 1)\n",
    ")\n",
    "personality_data[\"neg_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"neg_sentiment\"]).reshape(-1, 1)\n",
    ")\n",
    "personality_data[\"neu_sentiment\"] = min_max_scaler.fit_transform(\n",
    "    np.array(personality_data[\"neu_sentiment\"]).reshape(-1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type                  0\n",
       "is_Extrovert          0\n",
       "is_Sensing            0\n",
       "is_Thinking           0\n",
       "is_Judging            0\n",
       "posts                 0\n",
       "clean_posts           0\n",
       "compound_sentiment    0\n",
       "pos_sentiment         0\n",
       "neg_sentiment         0\n",
       "neu_sentiment         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking to see if sentiment scores introduced any null value\n",
    "personality_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tag_posts column that will have each post as a separate list in a row. tag_posts will be a list of 50 lists.\n",
    "\n",
    "# replacing urls with domain name\n",
    "personality_data[\"tag_posts\"] = personality_data[\"posts\"].str.replace(\n",
    "    re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\"),\n",
    "    lambda match: match.group(2),\n",
    ")\n",
    "\n",
    "# replacing ||| with space\n",
    "personality_data[\"tag_posts\"] = [\n",
    "    post for post in personality_data[\"tag_posts\"].str.split(\"\\|\\|\\|\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CAUTION - The next step i.e. Parts of speech tagging for each word will take SUPER LONG !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Time: 2230.3588178157806 seconds\n"
     ]
    }
   ],
   "source": [
    "# parts of speech tagging for each word\n",
    "t = time.time()\n",
    "\n",
    "personality_data[\"tagged_words\"] = personality_data[\"tag_posts\"].apply(\n",
    "    lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\n",
    ")\n",
    "\n",
    "print(f\"POS Tagging Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huawei D15\\AppData\\Local\\Temp\\ipykernel_18108\\2273190258.py:4: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for i, data in personality_data[\"tagged_words\"].iteritems():\n"
     ]
    }
   ],
   "source": [
    "# creating list of unique POS tags\n",
    "tag_set = set()\n",
    "\n",
    "for i, data in personality_data[\"tagged_words\"].iteritems():\n",
    "    for tup in data[0]:\n",
    "        tag_set.add(tup[1])\n",
    "\n",
    "tag_list = list(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Stats Time: 321.5249733924866 seconds\n"
     ]
    }
   ],
   "source": [
    "# calculating mean and standard deviation of pos tags for each user\n",
    "t = time.time()\n",
    "\n",
    "\n",
    "def pos_cat(x, tag):\n",
    "    return [len([y for y in line if y[1] == tag]) for line in x]\n",
    "\n",
    "\n",
    "for col in tag_list:\n",
    "    personality_data[\"POS_\" + col + \"_mean\"] = personality_data[\"tagged_words\"].apply(\n",
    "        lambda x: np.mean(pos_cat(x, col))\n",
    "    )\n",
    "    personality_data[\"POS_\" + col + \"_std\"] = personality_data[\"tagged_words\"].apply(\n",
    "        lambda x: np.std(pos_cat(x, col))\n",
    "    )\n",
    "\n",
    "print(f\"POS Stats Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping pos tags based on stanford list\n",
    "tags_dict = {\n",
    "    \"ADJ\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "    \"ADP\": [\"EX\", \"TO\"],\n",
    "    \"ADV\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "    \"CONJ\": [\"CC\", \"IN\"],\n",
    "    \"DET\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "    \"NOUN\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "    \"NUM\": [\"CD\"],\n",
    "    \"PRT\": [\"RP\"],\n",
    "    \"PRON\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "    \"VERB\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "    \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "    \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huawei D15\\AppData\\Local\\Temp\\ipykernel_18108\\2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\Huawei D15\\AppData\\Local\\Temp\\ipykernel_18108\\2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\Huawei D15\\AppData\\Local\\Temp\\ipykernel_18108\\2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\Huawei D15\\AppData\\Local\\Temp\\ipykernel_18108\\2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\Huawei D15\\AppData\\Local\\Temp\\ipykernel_18108\\2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\Huawei D15\\AppData\\Local\\Temp\\ipykernel_18108\\2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\Huawei D15\\AppData\\Local\\Temp\\ipykernel_18108\\2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\Huawei D15\\AppData\\Local\\Temp\\ipykernel_18108\\2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
      "C:\\Users\\Huawei D15\\AppData\\Local\\Temp\\ipykernel_18108\\2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford POS Stats Time: 91.40581035614014 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huawei D15\\AppData\\Local\\Temp\\ipykernel_18108\\2397153783.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n"
     ]
    }
   ],
   "source": [
    "# Stanford POS tag stats\n",
    "t = time.time()\n",
    "\n",
    "\n",
    "def stanford_tag(x, tag):\n",
    "    tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\n",
    "    return tags_list\n",
    "\n",
    "\n",
    "for col in tags_dict.keys():\n",
    "    personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
    "        lambda x: np.median(stanford_tag(x, col))\n",
    "    )\n",
    "\n",
    "print(f\"Stanford POS Stats Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>is_Extrovert</th>\n",
       "      <th>is_Sensing</th>\n",
       "      <th>is_Thinking</th>\n",
       "      <th>is_Judging</th>\n",
       "      <th>posts</th>\n",
       "      <th>clean_posts</th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>pos_sentiment</th>\n",
       "      <th>neg_sentiment</th>\n",
       "      <th>...</th>\n",
       "      <th>ADV_avg</th>\n",
       "      <th>CONJ_avg</th>\n",
       "      <th>DET_avg</th>\n",
       "      <th>NOUN_avg</th>\n",
       "      <th>NUM_avg</th>\n",
       "      <th>PRT_avg</th>\n",
       "      <th>PRON_avg</th>\n",
       "      <th>VERB_avg</th>\n",
       "      <th>._avg</th>\n",
       "      <th>X_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'When asked of the things you wish you did ear...</td>\n",
       "      <td>when asked  the things you wish you did earli...</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>0.346041</td>\n",
       "      <td>0.128155</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'I love both and they are equally important to...</td>\n",
       "      <td>love both and they are equally important    ...</td>\n",
       "      <td>0.99995</td>\n",
       "      <td>0.532258</td>\n",
       "      <td>0.143689</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  is_Extrovert  is_Sensing  is_Thinking  is_Judging  \\\n",
       "0  INFJ             0           0            0           1   \n",
       "1  INFJ             0           0            0           1   \n",
       "\n",
       "                                               posts  \\\n",
       "0  'When asked of the things you wish you did ear...   \n",
       "1  'I love both and they are equally important to...   \n",
       "\n",
       "                                         clean_posts  compound_sentiment  \\\n",
       "0   when asked  the things you wish you did earli...             0.99990   \n",
       "1    love both and they are equally important    ...             0.99995   \n",
       "\n",
       "   pos_sentiment  neg_sentiment  ...  ADV_avg CONJ_avg DET_avg  NOUN_avg  \\\n",
       "0       0.346041       0.128155  ...      4.0      5.0     3.0       6.0   \n",
       "1       0.532258       0.143689  ...      3.0      5.0     2.0       5.0   \n",
       "\n",
       "   NUM_avg  PRT_avg  PRON_avg  VERB_avg  ._avg  X_avg  \n",
       "0      0.0      0.0       4.0       8.0    5.0    0.0  \n",
       "1      0.0      0.0       5.0       8.0    3.0    0.0  \n",
       "\n",
       "[2 rows x 115 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a quick look at the data\n",
    "personality_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"# Sentiment scoring & POS Tagging took long. So saving the scored & tagged file to save time in the next step.\\npersonality_data.to_csv(os.path.join(\\\"..\\\", \\\"data\\\", \\\"clean_data_2.csv\\\"), index=False)\";\n                var nbb_formatted_code = \"# Sentiment scoring & POS Tagging took long. So saving the scored & tagged file to save time in the next step.\\npersonality_data.to_csv(os.path.join(\\\"..\\\", \\\"data\\\", \\\"clean_data_2.csv\\\"), index=False)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sentiment scoring & POS Tagging took long. So saving the scored & tagged file to save time in the next step.\n",
    "personality_data.to_csv(os.path.join(\"..\", \"data\", \"clean_data_2.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cdba3d1713347338e6fa1455c6c3086d5bdeaa151e7e53a03f3a0575feb8c6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
